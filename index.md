---
layout: page
title: Real-Gradients
tagline: Description and Resources
description: Website for Real-Gradients
---


Welcome to the website related to the paper _"Real Attackers Don't Compute Gradients": Bridging the Gap between Adversarial ML Research and Practice_ accepted to [IEEE SaTML'23](https://satml.org/).

We will [present the paper](https://satml.org/) in Raleigh (NC, USA) in February 2023!


---

### Summary: what is our paper about?

Our (position) paper aims to spearhead more impactful research in the context of Adversarial Machine Learning (ML). In the last decade, real-world deployments of ML have skyrocketed; however, despite thousands of papers showing the vulnerability of ML models to various security violations, practitioners still see this research domain with skepticism. 

We believe that a stronger connection between research and practice of adversarial ML would greatly benefit our future society, as it will lead to an improved security of _operational_ ML systems. To this purpose, we:
* **elucidate three real-world case studies**, fostering the contribution of large companies, elucidating some aspects of ML-systems' security that are _overlooked_ by researchers;
* **review all recent papers published in top-conferences**, showing positive trends as well as some confusing _inconsistencies_;
* **state five positions** that, if embraced, would _build a bridge_ between adversarial ML research and practice. 

Our paper is the result of a joint effort of researchers and practitioners. However, the _idea_ of our paper was born slightly after the Dagstuhl Seminar in "[Security of Machine Learning](https://www.dagstuhl.de/en/program/calendar/semhp/?semnr=22281)", held in July 2022. During this event (which was attended by most of the paper's authors), many discussions were held on the underlying topic tackled by the paper. The authors hence would like to thank all participants of this Dagstuhl Seminar, without which our paper would have never come to be.



---

### Resources

Alongside our main paper, our contributions also cover the following additional resources:



If you use any of such resources, we kindly ask you to cite our paper with the following BibTeX entry:
```
@inproceedings{apruzzese2023realgradients,
  title={"Real Attackers Don't Compute Gradients": Bridging the Gap between Adversarial ML Research and Practice},
  author={Apruzzese, Giovanni and Anderson, Hyrum S. and Dambra, Savino and Freeman, David and Pierazzi, Fabio and Roundy, Kevin A.},
  booktitle={Proceedings of the 1st IEEE Conference on Secure and Trustworthy Machine Learning (SaTML)},
  year={2023},
} 
```

#### Contact
Feel free to reach out to us! You can contact either the primary author, [Giovanni Apruzzese](mailto:giovanni.apruzzese@uni.li) or any of the other authors (the contact references are at the bottom of this webpage). You can also post a comment on the [discussion page](https://github.com/real-gradients/real-gradients.github.io/discussions/) of this GitHub Repository.